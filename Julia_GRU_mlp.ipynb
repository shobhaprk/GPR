{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>ID</th><th>Text</th><th>Pronoun</th><th>Pronoun_offset</th><th>A</th><th>A_offset</th><th>A_coref</th><th>B</th><th>B_offset</th><th>B_coref</th><th>URL</th></tr><tr><th></th><th>String</th><th>String</th><th>String</th><th>Int64</th><th>String</th><th>Int64</th><th>String</th><th>String</th><th>Int64</th><th>String</th><th>String</th></tr></thead><tbody><p>6 rows × 11 columns</p><tr><th>1</th><td>development-1</td><td>Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.</td><td>her</td><td>274</td><td>Cheryl Cassidy</td><td>191</td><td>TRUE</td><td>Pauline</td><td>207</td><td>FALSE</td><td>http://en.wikipedia.org/wiki/List_of_Teachers_(UK_TV_series)_characters</td></tr><tr><th>2</th><td>development-2</td><td>He grew up in Evanston, Illinois the second oldest of five children including his brothers, Fred and Gordon and sisters, Marge (Peppy) and Marilyn. His high school days were spent at New Trier High School in Winnetka, Illinois. MacKenzie studied with Bernard Leach from 1949 to 1952. His simple, wheel-thrown functional pottery is heavily influenced by the oriental aesthetic of Shoji Hamada and Kanjiro Kawai.</td><td>His</td><td>284</td><td>MacKenzie</td><td>228</td><td>TRUE</td><td>Bernard Leach</td><td>251</td><td>FALSE</td><td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td></tr><tr><th>3</th><td>development-3</td><td>He had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil. De la Sota again ran for governor of C*rdoba in 1991. Defeated by Governor Angeloz by over 15%, this latter setback was significant because it cost De la Sota much of his support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Carlos Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress.</td><td>his</td><td>265</td><td>Angeloz</td><td>173</td><td>FALSE</td><td>De la Sota</td><td>246</td><td>TRUE</td><td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_de_la_Sota</td></tr><tr><th>4</th><td>development-4</td><td>The current members of Crime have also performed in San Francisco under the band name ''Remote Viewers``. Strike has published two works of fiction in recent years: Ports of Hell, which is listed in the Rock and Roll Hall of Fame Library, and A Loud Humming Sound Came from Above. Rank has produced numerous films (under his real name, Henry Rosenthal) including the hit The Devil and Daniel Johnston.</td><td>his</td><td>321</td><td>Hell</td><td>174</td><td>FALSE</td><td>Henry Rosenthal</td><td>336</td><td>TRUE</td><td>http://en.wikipedia.org/wiki/Crime_(band)</td></tr><tr><th>5</th><td>development-5</td><td>Her Santa Fe Opera debut in 2005 was as Nuria in the revised edition of Golijov's Ainadamar. She sang on the subsequent Deutsche Grammophon recording of the opera. For his opera Doctor Atomic, Adams rewrote the role of Kitty Oppenheimer, originally a mezzo-soprano role, for soprano voice, and Rivera sang the rewritten part of Kitty Oppenheimer at Lyric Opera of Chicago, De Nederlandse Opera, and the Metropolitan Opera., all in 2007. She has since sung several parts and roles in John Adams' works, including the soprano part in El Ni*o, and the role of Kumudha in A Flowering Tree in the Peter Sellars production at the New Crowned Hope Festival in Vienna.</td><td>She</td><td>437</td><td>Kitty Oppenheimer</td><td>219</td><td>FALSE</td><td>Rivera</td><td>294</td><td>TRUE</td><td>http://en.wikipedia.org/wiki/Jessica_Rivera</td></tr><tr><th>6</th><td>development-6</td><td>Sandra Collins is an American DJ. She got her start on the West Coast of the U.S. in Phoenix, Arizona and into residencies in Los Angeles, and eventually moved towards trance. She used American producers to give herself a unique sound. Collins performed for an estimated 80,000 people on the first night of Woodstock '99, and was the first female DJ featured in the Tranceport series of influential recordings. She recently has released two CD mixes under Paul Oakenfold's Perfecto label.</td><td>She</td><td>411</td><td>Collins</td><td>236</td><td>TRUE</td><td>DJ</td><td>347</td><td>FALSE</td><td>http://en.wikipedia.org/wiki/Sandra_Collins</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccccc}\n",
       "\t& ID & Text & Pronoun & Pronoun\\_offset & A & A\\_offset & A\\_coref & B & B\\_offset & B\\_coref & URL\\\\\n",
       "\t\\hline\n",
       "\t& String & String & String & Int64 & String & Int64 & String & String & Int64 & String & String\\\\\n",
       "\t\\hline\n",
       "\t1 & development-1 & Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline. & her & 274 & Cheryl Cassidy & 191 & TRUE & Pauline & 207 & FALSE & http://en.wikipedia.org/wiki/List\\_of\\_Teachers\\_(UK\\_TV\\_series)\\_characters \\\\\n",
       "\t2 & development-2 & He grew up in Evanston, Illinois the second oldest of five children including his brothers, Fred and Gordon and sisters, Marge (Peppy) and Marilyn. His high school days were spent at New Trier High School in Winnetka, Illinois. MacKenzie studied with Bernard Leach from 1949 to 1952. His simple, wheel-thrown functional pottery is heavily influenced by the oriental aesthetic of Shoji Hamada and Kanjiro Kawai. & His & 284 & MacKenzie & 228 & TRUE & Bernard Leach & 251 & FALSE & http://en.wikipedia.org/wiki/Warren\\_MacKenzie \\\\\n",
       "\t3 & development-3 & He had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil. De la Sota again ran for governor of C*rdoba in 1991. Defeated by Governor Angeloz by over 15\\%, this latter setback was significant because it cost De la Sota much of his support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Carlos Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress. & his & 265 & Angeloz & 173 & FALSE & De la Sota & 246 & TRUE & http://en.wikipedia.org/wiki/Jos\\%C3\\%A9\\_Manuel\\_de\\_la\\_Sota \\\\\n",
       "\t4 & development-4 & The current members of Crime have also performed in San Francisco under the band name ''Remote Viewers``. Strike has published two works of fiction in recent years: Ports of Hell, which is listed in the Rock and Roll Hall of Fame Library, and A Loud Humming Sound Came from Above. Rank has produced numerous films (under his real name, Henry Rosenthal) including the hit The Devil and Daniel Johnston. & his & 321 & Hell & 174 & FALSE & Henry Rosenthal & 336 & TRUE & http://en.wikipedia.org/wiki/Crime\\_(band) \\\\\n",
       "\t5 & development-5 & Her Santa Fe Opera debut in 2005 was as Nuria in the revised edition of Golijov's Ainadamar. She sang on the subsequent Deutsche Grammophon recording of the opera. For his opera Doctor Atomic, Adams rewrote the role of Kitty Oppenheimer, originally a mezzo-soprano role, for soprano voice, and Rivera sang the rewritten part of Kitty Oppenheimer at Lyric Opera of Chicago, De Nederlandse Opera, and the Metropolitan Opera., all in 2007. She has since sung several parts and roles in John Adams' works, including the soprano part in El Ni*o, and the role of Kumudha in A Flowering Tree in the Peter Sellars production at the New Crowned Hope Festival in Vienna. & She & 437 & Kitty Oppenheimer & 219 & FALSE & Rivera & 294 & TRUE & http://en.wikipedia.org/wiki/Jessica\\_Rivera \\\\\n",
       "\t6 & development-6 & Sandra Collins is an American DJ. She got her start on the West Coast of the U.S. in Phoenix, Arizona and into residencies in Los Angeles, and eventually moved towards trance. She used American producers to give herself a unique sound. Collins performed for an estimated 80,000 people on the first night of Woodstock '99, and was the first female DJ featured in the Tranceport series of influential recordings. She recently has released two CD mixes under Paul Oakenfold's Perfecto label. & She & 411 & Collins & 236 & TRUE & DJ & 347 & FALSE & http://en.wikipedia.org/wiki/Sandra\\_Collins \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "6×11 DataFrame. Omitted printing of 10 columns\n",
       "│ Row │ ID            │\n",
       "│     │ \u001b[90mString\u001b[39m        │\n",
       "├─────┼───────────────┤\n",
       "│ 1   │ development-1 │\n",
       "│ 2   │ development-2 │\n",
       "│ 3   │ development-3 │\n",
       "│ 4   │ development-4 │\n",
       "│ 5   │ development-5 │\n",
       "│ 6   │ development-6 │"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV,DataFrames\n",
    "df = DataFrame(CSV.read(\"data/gap-development.tsv\",delim='\\t', allowmissing=:none, normalizenames=true));\n",
    "first(data,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Pronoun_offset</th><th>A_offset</th><th>B_offset</th><th>section_min</th><th>Pronoun_offset2</th><th>A_offset2</th><th>B_offset2</th><th>section_max</th><th>A_dist</th><th>B_dist</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>2,000 rows × 10 columns</p><tr><th>1</th><td>274</td><td>191</td><td>207</td><td>191</td><td>277</td><td>205</td><td>214</td><td>277</td><td>83</td><td>67</td></tr><tr><th>2</th><td>284</td><td>228</td><td>251</td><td>228</td><td>287</td><td>237</td><td>264</td><td>287</td><td>56</td><td>33</td></tr><tr><th>3</th><td>265</td><td>173</td><td>246</td><td>173</td><td>268</td><td>180</td><td>256</td><td>268</td><td>92</td><td>19</td></tr><tr><th>4</th><td>321</td><td>174</td><td>336</td><td>174</td><td>324</td><td>178</td><td>351</td><td>351</td><td>147</td><td>15</td></tr><tr><th>5</th><td>437</td><td>219</td><td>294</td><td>219</td><td>440</td><td>236</td><td>300</td><td>440</td><td>218</td><td>143</td></tr><tr><th>6</th><td>411</td><td>236</td><td>347</td><td>236</td><td>414</td><td>243</td><td>349</td><td>414</td><td>175</td><td>64</td></tr><tr><th>7</th><td>273</td><td>152</td><td>253</td><td>152</td><td>276</td><td>161</td><td>264</td><td>276</td><td>121</td><td>20</td></tr><tr><th>8</th><td>337</td><td>173</td><td>377</td><td>173</td><td>340</td><td>181</td><td>393</td><td>393</td><td>164</td><td>40</td></tr><tr><th>9</th><td>246</td><td>255</td><td>267</td><td>246</td><td>249</td><td>265</td><td>273</td><td>273</td><td>9</td><td>21</td></tr><tr><th>10</th><td>329</td><td>196</td><td>226</td><td>196</td><td>332</td><td>214</td><td>242</td><td>332</td><td>133</td><td>103</td></tr><tr><th>11</th><td>300</td><td>168</td><td>212</td><td>168</td><td>303</td><td>184</td><td>223</td><td>303</td><td>132</td><td>88</td></tr><tr><th>12</th><td>304</td><td>217</td><td>285</td><td>217</td><td>307</td><td>224</td><td>302</td><td>307</td><td>87</td><td>19</td></tr><tr><th>13</th><td>293</td><td>247</td><td>273</td><td>247</td><td>296</td><td>255</td><td>289</td><td>296</td><td>46</td><td>20</td></tr><tr><th>14</th><td>340</td><td>275</td><td>297</td><td>275</td><td>343</td><td>290</td><td>310</td><td>343</td><td>65</td><td>43</td></tr><tr><th>15</th><td>250</td><td>219</td><td>235</td><td>219</td><td>253</td><td>224</td><td>241</td><td>253</td><td>31</td><td>15</td></tr><tr><th>16</th><td>295</td><td>192</td><td>219</td><td>192</td><td>298</td><td>204</td><td>228</td><td>298</td><td>103</td><td>76</td></tr><tr><th>17</th><td>384</td><td>334</td><td>365</td><td>334</td><td>387</td><td>341</td><td>372</td><td>387</td><td>50</td><td>19</td></tr><tr><th>18</th><td>330</td><td>181</td><td>235</td><td>181</td><td>333</td><td>193</td><td>246</td><td>333</td><td>149</td><td>95</td></tr><tr><th>19</th><td>335</td><td>281</td><td>296</td><td>281</td><td>338</td><td>287</td><td>299</td><td>338</td><td>54</td><td>39</td></tr><tr><th>20</th><td>476</td><td>330</td><td>381</td><td>330</td><td>479</td><td>337</td><td>387</td><td>479</td><td>146</td><td>95</td></tr><tr><th>21</th><td>234</td><td>113</td><td>150</td><td>113</td><td>236</td><td>125</td><td>153</td><td>236</td><td>121</td><td>84</td></tr><tr><th>22</th><td>304</td><td>317</td><td>338</td><td>304</td><td>307</td><td>321</td><td>349</td><td>349</td><td>13</td><td>34</td></tr><tr><th>23</th><td>430</td><td>296</td><td>386</td><td>296</td><td>433</td><td>301</td><td>399</td><td>433</td><td>134</td><td>44</td></tr><tr><th>24</th><td>447</td><td>273</td><td>301</td><td>273</td><td>450</td><td>288</td><td>308</td><td>450</td><td>174</td><td>146</td></tr><tr><th>25</th><td>329</td><td>210</td><td>266</td><td>210</td><td>332</td><td>217</td><td>279</td><td>332</td><td>119</td><td>63</td></tr><tr><th>26</th><td>299</td><td>92</td><td>266</td><td>92</td><td>302</td><td>110</td><td>272</td><td>302</td><td>207</td><td>33</td></tr><tr><th>27</th><td>280</td><td>238</td><td>263</td><td>238</td><td>283</td><td>242</td><td>269</td><td>283</td><td>42</td><td>17</td></tr><tr><th>28</th><td>194</td><td>204</td><td>223</td><td>194</td><td>197</td><td>208</td><td>227</td><td>227</td><td>10</td><td>29</td></tr><tr><th>29</th><td>250</td><td>153</td><td>189</td><td>153</td><td>253</td><td>166</td><td>202</td><td>253</td><td>97</td><td>61</td></tr><tr><th>30</th><td>294</td><td>241</td><td>258</td><td>241</td><td>297</td><td>245</td><td>275</td><td>297</td><td>53</td><td>36</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Pronoun\\_offset & A\\_offset & B\\_offset & section\\_min & Pronoun\\_offset2 & A\\_offset2 & B\\_offset2 & section\\_max & A\\_dist & B\\_dist\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 274 & 191 & 207 & 191 & 277 & 205 & 214 & 277 & 83 & 67 \\\\\n",
       "\t2 & 284 & 228 & 251 & 228 & 287 & 237 & 264 & 287 & 56 & 33 \\\\\n",
       "\t3 & 265 & 173 & 246 & 173 & 268 & 180 & 256 & 268 & 92 & 19 \\\\\n",
       "\t4 & 321 & 174 & 336 & 174 & 324 & 178 & 351 & 351 & 147 & 15 \\\\\n",
       "\t5 & 437 & 219 & 294 & 219 & 440 & 236 & 300 & 440 & 218 & 143 \\\\\n",
       "\t6 & 411 & 236 & 347 & 236 & 414 & 243 & 349 & 414 & 175 & 64 \\\\\n",
       "\t7 & 273 & 152 & 253 & 152 & 276 & 161 & 264 & 276 & 121 & 20 \\\\\n",
       "\t8 & 337 & 173 & 377 & 173 & 340 & 181 & 393 & 393 & 164 & 40 \\\\\n",
       "\t9 & 246 & 255 & 267 & 246 & 249 & 265 & 273 & 273 & 9 & 21 \\\\\n",
       "\t10 & 329 & 196 & 226 & 196 & 332 & 214 & 242 & 332 & 133 & 103 \\\\\n",
       "\t11 & 300 & 168 & 212 & 168 & 303 & 184 & 223 & 303 & 132 & 88 \\\\\n",
       "\t12 & 304 & 217 & 285 & 217 & 307 & 224 & 302 & 307 & 87 & 19 \\\\\n",
       "\t13 & 293 & 247 & 273 & 247 & 296 & 255 & 289 & 296 & 46 & 20 \\\\\n",
       "\t14 & 340 & 275 & 297 & 275 & 343 & 290 & 310 & 343 & 65 & 43 \\\\\n",
       "\t15 & 250 & 219 & 235 & 219 & 253 & 224 & 241 & 253 & 31 & 15 \\\\\n",
       "\t16 & 295 & 192 & 219 & 192 & 298 & 204 & 228 & 298 & 103 & 76 \\\\\n",
       "\t17 & 384 & 334 & 365 & 334 & 387 & 341 & 372 & 387 & 50 & 19 \\\\\n",
       "\t18 & 330 & 181 & 235 & 181 & 333 & 193 & 246 & 333 & 149 & 95 \\\\\n",
       "\t19 & 335 & 281 & 296 & 281 & 338 & 287 & 299 & 338 & 54 & 39 \\\\\n",
       "\t20 & 476 & 330 & 381 & 330 & 479 & 337 & 387 & 479 & 146 & 95 \\\\\n",
       "\t21 & 234 & 113 & 150 & 113 & 236 & 125 & 153 & 236 & 121 & 84 \\\\\n",
       "\t22 & 304 & 317 & 338 & 304 & 307 & 321 & 349 & 349 & 13 & 34 \\\\\n",
       "\t23 & 430 & 296 & 386 & 296 & 433 & 301 & 399 & 433 & 134 & 44 \\\\\n",
       "\t24 & 447 & 273 & 301 & 273 & 450 & 288 & 308 & 450 & 174 & 146 \\\\\n",
       "\t25 & 329 & 210 & 266 & 210 & 332 & 217 & 279 & 332 & 119 & 63 \\\\\n",
       "\t26 & 299 & 92 & 266 & 92 & 302 & 110 & 272 & 302 & 207 & 33 \\\\\n",
       "\t27 & 280 & 238 & 263 & 238 & 283 & 242 & 269 & 283 & 42 & 17 \\\\\n",
       "\t28 & 194 & 204 & 223 & 194 & 197 & 208 & 227 & 227 & 10 & 29 \\\\\n",
       "\t29 & 250 & 153 & 189 & 153 & 253 & 166 & 202 & 253 & 97 & 61 \\\\\n",
       "\t30 & 294 & 241 & 258 & 241 & 297 & 245 & 275 & 297 & 53 & 36 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "2000×10 DataFrame. Omitted printing of 5 columns\n",
       "│ Row  │ Pronoun_offset │ A_offset │ B_offset │ section_min │ Pronoun_offset2 │\n",
       "│      │ \u001b[90mInt64\u001b[39m          │ \u001b[90mInt64\u001b[39m    │ \u001b[90mInt64\u001b[39m    │ \u001b[90mInt64\u001b[39m       │ \u001b[90mInt64\u001b[39m           │\n",
       "├──────┼────────────────┼──────────┼──────────┼─────────────┼─────────────────┤\n",
       "│ 1    │ 274            │ 191      │ 207      │ 191         │ 277             │\n",
       "│ 2    │ 284            │ 228      │ 251      │ 228         │ 287             │\n",
       "│ 3    │ 265            │ 173      │ 246      │ 173         │ 268             │\n",
       "│ 4    │ 321            │ 174      │ 336      │ 174         │ 324             │\n",
       "│ 5    │ 437            │ 219      │ 294      │ 219         │ 440             │\n",
       "│ 6    │ 411            │ 236      │ 347      │ 236         │ 414             │\n",
       "│ 7    │ 273            │ 152      │ 253      │ 152         │ 276             │\n",
       "│ 8    │ 337            │ 173      │ 377      │ 173         │ 340             │\n",
       "│ 9    │ 246            │ 255      │ 267      │ 246         │ 249             │\n",
       "│ 10   │ 329            │ 196      │ 226      │ 196         │ 332             │\n",
       "⋮\n",
       "│ 1990 │ 414            │ 355      │ 370      │ 355         │ 417             │\n",
       "│ 1991 │ 301            │ 264      │ 277      │ 264         │ 304             │\n",
       "│ 1992 │ 318            │ 101      │ 128      │ 101         │ 321             │\n",
       "│ 1993 │ 316            │ 343      │ 372      │ 316         │ 319             │\n",
       "│ 1994 │ 407            │ 363      │ 384      │ 363         │ 410             │\n",
       "│ 1995 │ 212            │ 131      │ 147      │ 131         │ 215             │\n",
       "│ 1996 │ 433            │ 255      │ 328      │ 255         │ 436             │\n",
       "│ 1997 │ 246            │ 111      │ 215      │ 111         │ 249             │\n",
       "│ 1998 │ 348            │ 259      │ 266      │ 259         │ 351             │\n",
       "│ 1999 │ 284            │ 145      │ 208      │ 145         │ 287             │\n",
       "│ 2000 │ 373            │ 293      │ 347      │ 293         │ 376             │"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:section_min] = min.(df[:Pronoun_offset], df[:A_offset],df[:B_offset])\n",
    "df[:Pronoun_offset2] = df[:Pronoun_offset] .+ length.(df[:Pronoun])\n",
    "df[:A_offset2] = df[:A_offset] .+ length.(df[:A])\n",
    "df[:B_offset2] = df[:B_offset] .+ length.(df[:B])\n",
    "df[:section_max] = max.(df[:Pronoun_offset2], df[:A_offset2],df[:B_offset2])\n",
    "df[:Text] = replace.(df[:Text],df[:A].=>\"subjecta\")\n",
    "df[:Text] = replace.(df[:Text],df[:B].=>\"subjectb\")\n",
    "df[:A_dist] = abs.(df[:Pronoun_offset] .- df[:A_offset])\n",
    "df[:B_dist] = abs.(df[:Pronoun_offset] .- df[:B_offset]);\n",
    "train = df[:,[:Pronoun_offset,:A_offset,:B_offset,:section_min,:Pronoun_offset2,:A_offset2,:B_offset2,:section_max,:A_dist,:B_dist]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000-element Array{Array{Int64,1},1}:\n",
       " [274, 191, 207, 191, 277, 205, 214, 277, 83, 67]  \n",
       " [284, 228, 251, 228, 287, 237, 264, 287, 56, 33]  \n",
       " [265, 173, 246, 173, 268, 180, 256, 268, 92, 19]  \n",
       " [321, 174, 336, 174, 324, 178, 351, 351, 147, 15] \n",
       " [437, 219, 294, 219, 440, 236, 300, 440, 218, 143]\n",
       " [411, 236, 347, 236, 414, 243, 349, 414, 175, 64] \n",
       " [273, 152, 253, 152, 276, 161, 264, 276, 121, 20] \n",
       " [337, 173, 377, 173, 340, 181, 393, 393, 164, 40] \n",
       " [246, 255, 267, 246, 249, 265, 273, 273, 9, 21]   \n",
       " [329, 196, 226, 196, 332, 214, 242, 332, 133, 103]\n",
       " [300, 168, 212, 168, 303, 184, 223, 303, 132, 88] \n",
       " [304, 217, 285, 217, 307, 224, 302, 307, 87, 19]  \n",
       " [293, 247, 273, 247, 296, 255, 289, 296, 46, 20]  \n",
       " ⋮                                                 \n",
       " [81, 31, 54, 31, 84, 35, 58, 84, 50, 27]          \n",
       " [414, 355, 370, 355, 417, 363, 388, 417, 59, 44]  \n",
       " [301, 264, 277, 264, 304, 272, 285, 304, 37, 24]  \n",
       " [318, 101, 128, 101, 321, 106, 132, 321, 217, 190]\n",
       " [316, 343, 372, 316, 319, 348, 376, 376, 27, 56]  \n",
       " [407, 363, 384, 363, 410, 377, 396, 410, 44, 23]  \n",
       " [212, 131, 147, 131, 215, 141, 162, 215, 81, 65]  \n",
       " [433, 255, 328, 255, 436, 261, 332, 436, 178, 105]\n",
       " [246, 111, 215, 111, 249, 120, 218, 249, 135, 31] \n",
       " [348, 259, 266, 259, 351, 264, 281, 351, 89, 82]  \n",
       " [284, 145, 208, 145, 287, 150, 223, 287, 139, 76] \n",
       " [373, 293, 347, 293, 376, 302, 354, 376, 80, 26]  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = [ [df[i,:Pronoun_offset],df[i,:A_offset],df[i,:B_offset],df[i,:section_min],df[i,:Pronoun_offset2],df[i,:A_offset2],df[i,:B_offset2],df[i,:section_max],df[i,:A_dist],df[i,:B_dist]] for i in 1:size(df)[1] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000-element Array{Array{Float32,1},1}:\n",
       " [2.74, 1.91, 2.07, 1.91, 2.77, 2.05, 2.14, 2.77, 0.83, 0.67]\n",
       " [2.84, 2.28, 2.51, 2.28, 2.87, 2.37, 2.64, 2.87, 0.56, 0.33]\n",
       " [2.65, 1.73, 2.46, 1.73, 2.68, 1.8, 2.56, 2.68, 0.92, 0.19] \n",
       " [3.21, 1.74, 3.36, 1.74, 3.24, 1.78, 3.51, 3.51, 1.47, 0.15]\n",
       " [4.37, 2.19, 2.94, 2.19, 4.4, 2.36, 3.0, 4.4, 2.18, 1.43]   \n",
       " [4.11, 2.36, 3.47, 2.36, 4.14, 2.43, 3.49, 4.14, 1.75, 0.64]\n",
       " [2.73, 1.52, 2.53, 1.52, 2.76, 1.61, 2.64, 2.76, 1.21, 0.2] \n",
       " [3.37, 1.73, 3.77, 1.73, 3.4, 1.81, 3.93, 3.93, 1.64, 0.4]  \n",
       " [2.46, 2.55, 2.67, 2.46, 2.49, 2.65, 2.73, 2.73, 0.09, 0.21]\n",
       " [3.29, 1.96, 2.26, 1.96, 3.32, 2.14, 2.42, 3.32, 1.33, 1.03]\n",
       " [3.0, 1.68, 2.12, 1.68, 3.03, 1.84, 2.23, 3.03, 1.32, 0.88] \n",
       " [3.04, 2.17, 2.85, 2.17, 3.07, 2.24, 3.02, 3.07, 0.87, 0.19]\n",
       " [2.93, 2.47, 2.73, 2.47, 2.96, 2.55, 2.89, 2.96, 0.46, 0.2] \n",
       " ⋮                                                           \n",
       " [0.81, 0.31, 0.54, 0.31, 0.84, 0.35, 0.58, 0.84, 0.5, 0.27] \n",
       " [4.14, 3.55, 3.7, 3.55, 4.17, 3.63, 3.88, 4.17, 0.59, 0.44] \n",
       " [3.01, 2.64, 2.77, 2.64, 3.04, 2.72, 2.85, 3.04, 0.37, 0.24]\n",
       " [3.18, 1.01, 1.28, 1.01, 3.21, 1.06, 1.32, 3.21, 2.17, 1.9] \n",
       " [3.16, 3.43, 3.72, 3.16, 3.19, 3.48, 3.76, 3.76, 0.27, 0.56]\n",
       " [4.07, 3.63, 3.84, 3.63, 4.1, 3.77, 3.96, 4.1, 0.44, 0.23]  \n",
       " [2.12, 1.31, 1.47, 1.31, 2.15, 1.41, 1.62, 2.15, 0.81, 0.65]\n",
       " [4.33, 2.55, 3.28, 2.55, 4.36, 2.61, 3.32, 4.36, 1.78, 1.05]\n",
       " [2.46, 1.11, 2.15, 1.11, 2.49, 1.2, 2.18, 2.49, 1.35, 0.31] \n",
       " [3.48, 2.59, 2.66, 2.59, 3.51, 2.64, 2.81, 3.51, 0.89, 0.82]\n",
       " [2.84, 1.45, 2.08, 1.45, 2.87, 1.5, 2.23, 2.87, 1.39, 0.76] \n",
       " [3.73, 2.93, 3.47, 2.93, 3.76, 3.02, 3.54, 3.76, 0.8, 0.26] "
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = DataFrame() \n",
    "labels[:A],labels[:B] = df[:A_coref],df[:B_coref];\n",
    "labels[:Neither] = zeros(2000)\n",
    "labels[:A] = map(x-> x ==\"TRUE\" ? 1 : 2, labels[:A])\n",
    "labels[:B] = map(x-> x ==\"TRUE\" ? true : false, labels[:B])\n",
    "labels[:Neither] = map(x-> x = false, labels[:Neither])\n",
    "\n",
    "#train_Y = [ [labels[i,:A],labels[i,:B],labels[i,:Neither]] for i in 1:size(labels)[1] ]\n",
    "train_Y = [ onehot(labels[i,:A], 1:3) for i in 1:size(labels)[1] ]\n",
    "\n",
    "train_XF = [Float32.(i)./100 for i in train_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"NNlib\")\n",
    "using Flux;\n",
    "using Flux:crossentropy,throttle\n",
    "using StatsBase\n",
    "\n",
    "\n",
    "model2 = Chain(Dense(10,5,relu),\n",
    "    Dense(5,3,NNlib.σ),softmax)\n",
    "\n",
    "\n",
    "    \n",
    "function loss(x,y)\n",
    "    x = train_XF\n",
    "    y = train_Y\n",
    "    i = sample(1:2000, 1, replace = false)[1]\n",
    "    l = Flux.mse(model2(x[i]),y[i])\n",
    "    l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Iterators.Zip2{Array{Array{Float32,1},1},Array{Flux.OneHotVector,1}}(Array{Float32,1}[[2.74, 1.91, 2.07, 1.91, 2.77, 2.05, 2.14, 2.77, 0.83, 0.67], [2.84, 2.28, 2.51, 2.28, 2.87, 2.37, 2.64, 2.87, 0.56, 0.33], [2.65, 1.73, 2.46, 1.73, 2.68, 1.8, 2.56, 2.68, 0.92, 0.19], [3.21, 1.74, 3.36, 1.74, 3.24, 1.78, 3.51, 3.51, 1.47, 0.15], [4.37, 2.19, 2.94, 2.19, 4.4, 2.36, 3.0, 4.4, 2.18, 1.43], [4.11, 2.36, 3.47, 2.36, 4.14, 2.43, 3.49, 4.14, 1.75, 0.64], [2.73, 1.52, 2.53, 1.52, 2.76, 1.61, 2.64, 2.76, 1.21, 0.2], [3.37, 1.73, 3.77, 1.73, 3.4, 1.81, 3.93, 3.93, 1.64, 0.4], [2.46, 2.55, 2.67, 2.46, 2.49, 2.65, 2.73, 2.73, 0.09, 0.21], [3.29, 1.96, 2.26, 1.96, 3.32, 2.14, 2.42, 3.32, 1.33, 1.03]  …  [3.01, 2.64, 2.77, 2.64, 3.04, 2.72, 2.85, 3.04, 0.37, 0.24], [3.18, 1.01, 1.28, 1.01, 3.21, 1.06, 1.32, 3.21, 2.17, 1.9], [3.16, 3.43, 3.72, 3.16, 3.19, 3.48, 3.76, 3.76, 0.27, 0.56], [4.07, 3.63, 3.84, 3.63, 4.1, 3.77, 3.96, 4.1, 0.44, 0.23], [2.12, 1.31, 1.47, 1.31, 2.15, 1.41, 1.62, 2.15, 0.81, 0.65], [4.33, 2.55, 3.28, 2.55, 4.36, 2.61, 3.32, 4.36, 1.78, 1.05], [2.46, 1.11, 2.15, 1.11, 2.49, 1.2, 2.18, 2.49, 1.35, 0.31], [3.48, 2.59, 2.66, 2.59, 3.51, 2.64, 2.81, 3.51, 0.89, 0.82], [2.84, 1.45, 2.08, 1.45, 2.87, 1.5, 2.23, 2.87, 1.39, 0.76], [3.73, 2.93, 3.47, 2.93, 3.76, 3.02, 3.54, 3.76, 0.8, 0.26]], Flux.OneHotVector[[true, false, false], [true, false, false], [false, true, false], [false, true, false], [false, true, false], [true, false, false], [false, true, false], [false, true, false], [false, true, false], [true, false, false]  …  [false, true, false], [false, true, false], [true, false, false], [true, false, false], [false, true, false], [false, true, false], [false, true, false], [true, false, false], [true, false, false], [false, true, false]])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_XY = zip(train_XF,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chain{Tuple{Dense{typeof(σ),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},Dense{typeof(σ),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},typeof(softmax)}}\""
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#305 (generic function with 1 method)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalcb = () -> @show loss(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(train_X, train_Y) = 0.30350205f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.21080504f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.18065402f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.16164085f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17522345f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.12552236f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.18585739f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17699604f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.21840727f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.169492f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17401221f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.16458485f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15969728f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17121929f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.28681237f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.27232027f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.21466964f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.19376385f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.25468892f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.16486222f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.14651164f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.119277716f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17766553f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17957833f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.10526996f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.22183292f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15693586f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13755617f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.27221435f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.23253328f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.14504394f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.18058813f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.12264054f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.29914585f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.12355851f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17846906f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15916495f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15979663f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.27487504f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13145569f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13092506f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.09965129f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.22862892f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17990981f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.177014f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.19612586f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17886244f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.22860268f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.1555823f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.16346928f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.14284118f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15923403f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.16337056f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.170377f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.24506007f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15708718f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.2533084f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.18519776f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.18008006f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.108240776f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17914811f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.14287078f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.11278987f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.26199052f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17149961f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17349839f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15912858f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.23960117f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15802333f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.22304176f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.22984263f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.19248629f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17816447f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13924839f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.16873553f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13347831f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.23466787f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17073661f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13946454f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13486297f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.24857007f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.14974621f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.12360117f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.1652573f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.18580848f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.3300701f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.15043831f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13259232f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.23557179f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.20853376f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.20656548f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.118051134f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.122609004f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.23244256f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.26088083f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.17932877f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.11217722f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.2403212f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.13539854f0 (tracked)\n",
      "loss(train_X, train_Y) = 0.122588575f0 (tracked)\n"
     ]
    }
   ],
   "source": [
    "for i  in 1:100\n",
    "Flux.train!(loss, params(model2),data_XY, Descent(0.05),cb = throttle(evalcb, 30))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 3-element Array{Float32,1}:\n",
       " 0.42231882f0\n",
       " 0.42231882f0\n",
       " 0.15536241f0"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(train_X[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Flux.OneHotVector:\n",
       " false\n",
       "  true\n",
       " false"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[1656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[2.66272 1.38406 … 3.3146 2.88083; 0.111684 -0.502158 … -0.893697 0.411466; … ; 0.537957 -0.414714 … -0.282643 -0.476555; -0.737402 -0.535208 … -0.910332 -0.169445] (tracked), Float32[2.50304, -0.520363, -0.0931673, 0.0, -0.429112] (tracked), Float32[2.17758 1.64441 … -0.744242 0.396827; 1.86046 0.86408 … 0.209833 -0.488413; -2.36168 -2.03426 … -0.305097 -0.373036] (tracked), Float32[1.4051, 2.20709, -1.62545] (tracked)])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
